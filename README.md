<h1 align="center">Flab-Pruner: Towards Greener Yet Powerful Code Intelligence via Structural Pruning</h1>

<p align="left">
    ‚≠êÔ∏è&nbsp;<a href="#about">About</a>
    | üöÄ&nbsp;<a href="#quick-start">Quick start</a>
    | üìä&nbsp;<a href="#evaluation">Evaluation</a>
    | ‚ö†Ô∏è&nbsp;<a href="#bias-risks-and-limitations">Limitations</a>
</p>

## About

We introduce Flab-Pruner, a structural pruning approach that enhances efficiency and sustainability without compromising the performance of Code LLMs.

- **CodeQwen1.5-Pruned Models:** [CodeQwen1.5-Pruned](https://huggingface.co/collections/Flab-Pruner/codeqwen15-pruned-668e8261e2a31183b33766c0)
- **Nxcode-CQ-Pruned Models:** [Nxcode-CQ-pruned](https://huggingface.co/collections/Flab-Pruner/nxcode-cq-pruned-668e82b60ff582ec89fcbb80)
- **Dataset:** [Flab-Pruner/CodeHarmony](https://huggingface.co/datasets/Flab-Pruner/CodeHarmony)


## Quick start

### Vocab Pruning

### FFN Pruning

### Layer Pruning

### Post Training

### How to use tuned model
Here is an example to get started with CodeQwen1.5-5.58B-Instruct-V1 using the [transformers](https://huggingface.co/docs/transformers/index) library:

```python
import transformers
import torch

```

## Evaluation

## Bias, Risks, and Limitations

## Cite as

```
@misc{Flab-Pruner,
  author       = {Flab-Pruner},
  title = {Flab-Pruner: Towards Greener Yet Powerful Code Intelligence via Structural Pruning},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Flab-Pruner/Flab-Pruner}},
  year = 2024,
}
